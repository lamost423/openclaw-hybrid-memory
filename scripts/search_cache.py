#!/usr/bin/env python3
"""
Search Result Cache - æœç´¢ç»“æœç¼“å­˜ç³»ç»Ÿ
ç¼“å­˜çƒ­é—¨æŸ¥è¯¢ç»“æœï¼ŒåŠ é€Ÿå“åº”
"""

import os
import json
import hashlib
import time
from pathlib import Path
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    query_hash: str
    query: str
    results: List[Dict]
    timestamp: float
    hit_count: int
    last_accessed: float

def get_workspace():
    return Path.home() / ".openclaw" / "workspace"

class SearchCache:
    """æœç´¢ç»“æœç¼“å­˜"""
    
    def __init__(self, max_size: int = 100, ttl_hours: int = 24):
        self.workspace = get_workspace()
        self.cache_dir = self.workspace / "config" / "self-memory" / "cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.cache_file = self.cache_dir / "search_cache.json"
        self.stats_file = self.cache_dir / "cache_stats.json"
        
        self.max_size = max_size
        self.ttl_seconds = ttl_hours * 3600
        
        self._cache = {}
        self._load_cache()
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data:
                        entry = CacheEntry(**item)
                        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                        if time.time() - entry.timestamp < self.ttl_seconds:
                            self._cache[entry.query_hash] = entry
                        else:
                            self._cache[entry.query_hash] = entry
            except:
                pass
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        # æ¸…ç†è¿‡æœŸæ¡ç›®
        current_time = time.time()
        expired = [
            k for k, v in self._cache.items()
            if current_time - v.timestamp > self.ttl_seconds
        ]
        for k in expired:
            del self._cache[k]
        
        # å¦‚æœè¶…å‡ºå¤§å°é™åˆ¶ï¼Œç§»é™¤æœ€å°‘ä½¿ç”¨çš„
        if len(self._cache) > self.max_size:
            # æŒ‰è®¿é—®æ—¶é—´å’Œå‘½ä¸­æ¬¡æ•°æ’åº
            sorted_items = sorted(
                self._cache.items(),
                key=lambda x: (x[1].hit_count, x[1].last_accessed),
                reverse=True
            )
            # ä¿ç•™å‰ max_size ä¸ª
            self._cache = dict(sorted_items[:self.max_size])
        
        # ä¿å­˜
        data = [asdict(v) for v in self._cache.values()]
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def _compute_hash(self, query: str) -> str:
        """è®¡ç®—æŸ¥è¯¢å“ˆå¸Œ"""
        # å½’ä¸€åŒ–æŸ¥è¯¢ï¼šå°å†™ã€å»ç©ºæ ¼
        normalized = ' '.join(query.lower().split())
        return hashlib.md5(normalized.encode()).hexdigest()[:16]
    
    def get(self, query: str) -> Optional[List[Dict]]:
        """è·å–ç¼“å­˜ç»“æœ"""
        query_hash = self._compute_hash(query)
        
        if query_hash in self._cache:
            entry = self._cache[query_hash]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - entry.timestamp > self.ttl_seconds:
                del self._cache[query_hash]
                self._update_stats(miss=True)
                return None
            
            # æ›´æ–°è®¿é—®ä¿¡æ¯
            entry.hit_count += 1
            entry.last_accessed = time.time()
            
            self._update_stats(hit=True)
            return entry.results
        
        self._update_stats(miss=True)
        return None
    
    def set(self, query: str, results: List[Dict]):
        """è®¾ç½®ç¼“å­˜"""
        query_hash = self._compute_hash(query)
        
        entry = CacheEntry(
            query_hash=query_hash,
            query=query,
            results=results,
            timestamp=time.time(),
            hit_count=1,
            last_accessed=time.time()
        )
        
        self._cache[query_hash] = entry
        self._save_cache()
        self._update_stats(add=True)
    
    def _update_stats(self, hit: bool = False, miss: bool = False, add: bool = False):
        """æ›´æ–°ç»Ÿè®¡"""
        stats = self._load_stats()
        
        if hit:
            stats["hits"] = stats.get("hits", 0) + 1
        if miss:
            stats["misses"] = stats.get("misses", 0) + 1
        if add:
            stats["adds"] = stats.get("adds", 0) + 1
        
        stats["total_queries"] = stats.get("total_queries", 0) + (1 if hit or miss else 0)
        stats["cache_size"] = len(self._cache)
        
        # è®¡ç®—å‘½ä¸­ç‡
        total = stats.get("hits", 0) + stats.get("misses", 0)
        if total > 0:
            stats["hit_rate"] = stats["hits"] / total
        
        with open(self.stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2)
    
    def _load_stats(self) -> Dict:
        """åŠ è½½ç»Ÿè®¡"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {"hits": 0, "misses": 0, "adds": 0, "total_queries": 0, "hit_rate": 0, "cache_size": 0}
    
    def invalidate(self, query: str = None):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        if query:
            query_hash = self._compute_hash(query)
            if query_hash in self._cache:
                del self._cache[query_hash]
        else:
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self._cache.clear()
        
        self._save_cache()
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        stats = self._load_stats()
        stats["current_cache_size"] = len(self._cache)
        stats["max_cache_size"] = self.max_size
        stats["ttl_hours"] = self.ttl_seconds / 3600
        return stats
    
    def get_popular_queries(self, limit: int = 10) -> List[tuple]:
        """è·å–çƒ­é—¨æŸ¥è¯¢"""
        sorted_items = sorted(
            self._cache.items(),
            key=lambda x: x[1].hit_count,
            reverse=True
        )
        
        return [
            (item.query, item.hit_count)
            for _, item in sorted_items[:limit]
        ]
    
    def clear_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired = [
            k for k, v in self._cache.items()
            if current_time - v.timestamp > self.ttl_seconds
        ]
        
        for k in expired:
            del self._cache[k]
        
        if expired:
            self._save_cache()
        
        return len(expired)

# å…¨å±€ç¼“å­˜å®ä¾‹
_search_cache = None

def get_search_cache() -> SearchCache:
    """è·å–æœç´¢ç¼“å­˜å•ä¾‹"""
    global _search_cache
    if _search_cache is None:
        _search_cache = SearchCache()
    return _search_cache

def cached_search(query: str, search_func, *args, **kwargs) -> List[Dict]:
    """
    å¸¦ç¼“å­˜çš„æœç´¢
    ç”¨æ³•:
        results = cached_search("query", hybrid_search, "query", top_k=5)
    """
    cache = get_search_cache()
    
    # å°è¯•ä»ç¼“å­˜è·å–
    cached_results = cache.get(query)
    if cached_results is not None:
        print(f"ğŸ¯ Cache hit for: {query[:30]}...")
        return cached_results
    
    # æ‰§è¡Œæœç´¢
    results = search_func(*args, **kwargs)
    
    # å­˜å…¥ç¼“å­˜
    cache.set(query, results)
    
    return results

# CLI æ¥å£
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Search Result Cache")
    parser.add_argument("--stats", action="store_true", help="Show cache statistics")
    parser.add_argument("--clear", action="store_true", help="Clear all cache")
    parser.add_argument("--clear-expired", action="store_true", help="Clear expired entries")
    parser.add_argument("--popular", action="store_true", help="Show popular queries")
    parser.add_argument("--invalidate", metavar="QUERY", help="Invalidate specific query")
    
    args = parser.parse_args()
    
    cache = get_search_cache()
    
    if args.stats:
        stats = cache.get_stats()
        print("ğŸ“Š Cache Statistics:")
        print("-" * 60)
        print(f"Total queries: {stats['total_queries']}")
        print(f"Cache hits: {stats['hits']}")
        print(f"Cache misses: {stats['misses']}")
        print(f"Hit rate: {stats['hit_rate']:.1%}")
        print(f"Current size: {stats['current_cache_size']} / {stats['max_cache_size']}")
        print(f"TTL: {stats['ttl_hours']:.0f} hours")
    
    elif args.clear:
        cache.invalidate()
        print("âœ… Cache cleared")
    
    elif args.clear_expired:
        count = cache.clear_expired()
        print(f"âœ… Cleared {count} expired entries")
    
    elif args.popular:
        popular = cache.get_popular_queries(10)
        print("ğŸ”¥ Popular Queries:")
        for query, hits in popular:
            print(f"  {hits}x {query[:50]}")
    
    elif args.invalidate:
        cache.invalidate(args.invalidate)
        print(f"âœ… Invalidated cache for: {args.invalidate}")
    
    else:
        parser.print_help()
