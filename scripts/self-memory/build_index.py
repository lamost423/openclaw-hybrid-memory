#!/usr/bin/env python3
"""
Build Index Script - ä¸º Self-Memory ç³»ç»Ÿåˆ›å»º BM25 + å‘é‡ç´¢å¼•
è¯»å– memory/ ç›®å½•æ‰€æœ‰ .md æ–‡ä»¶ï¼Œç”Ÿæˆç´¢å¼•ä¿å­˜åˆ° config/self-memory/index/
"""

import os
import sys
import json
import pickle
import hashlib
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime

# æ·»åŠ è™šæ‹Ÿç¯å¢ƒè·¯å¾„
venv_path = Path.home() / ".openclaw" / "venv"
if venv_path.exists():
    sys.path.insert(0, str(venv_path / "lib" / "python3.14" / "site-packages"))

from rank_bm25 import BM25Okapi


class IndexBuilder:
    """ç´¢å¼•æ„å»ºå™¨ - æ”¯æŒ BM25 å’Œå‘é‡ç´¢å¼•"""
    
    def __init__(self):
        self.workspace = Path.home() / ".openclaw" / "workspace"
        self.memory_dir = self.workspace / "memory"
        self.index_dir = self.workspace / "config" / "self-memory" / "index"
        self.ollama_url = "http://localhost:11434/api/embeddings"
        
        # ç¡®ä¿ç´¢å¼•ç›®å½•å­˜åœ¨
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡æ¡£å­˜å‚¨
        self.documents = []
        self.tokenized_corpus = []
        self.embeddings = []
        
    def tokenize(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯ï¼ˆå­—ç¬¦çº§ + è‹±æ–‡å•è¯æå–ï¼‰"""
        import re
        # æå–ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å•è¯å’Œæ•°å­—
        tokens = re.findall(r'[\u4e00-\u9fa5]|[a-zA-Z]+|\d+', text.lower())
        return tokens
    
    def get_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨ Ollama è·å–æ–‡æœ¬çš„ embedding"""
        import requests
        try:
            # é™åˆ¶æ–‡æœ¬é•¿åº¦é¿å…è¿‡é•¿
            text = text[:2000]
            response = requests.post(
                self.ollama_url,
                json={"model": "mxbai-embed-large", "prompt": text},
                timeout=60
            )
            response.raise_for_status()
            return response.json()["embedding"]
        except Exception as e:
            print(f"  âš ï¸  Embedding error: {e}")
            return []
    
    def load_documents(self) -> List[Dict]:
        """ä» memory ç›®å½•åŠ è½½æ‰€æœ‰ .md æ–‡ä»¶"""
        documents = []
        
        if not self.memory_dir.exists():
            print(f"âš ï¸  Memory directory not found: {self.memory_dir}")
            return documents
        
        for md_file in sorted(self.memory_dir.glob("*.md")):
            try:
                content = md_file.read_text(encoding="utf-8")
                stat = md_file.stat()
                
                documents.append({
                    "id": md_file.stem,
                    "filename": md_file.name,
                    "content": content,
                    "path": str(md_file.relative_to(self.workspace)),
                    "size": stat.st_size,
                    "mtime": stat.st_mtime,
                    "word_count": len(content.split())
                })
            except Exception as e:
                print(f"  âš ï¸  Error reading {md_file}: {e}")
        
        return documents
    
    def build_bm25_index(self) -> BM25Okapi:
        """æ„å»º BM25 ç´¢å¼•"""
        print("ğŸ”¨ Building BM25 index...")
        
        self.tokenized_corpus = []
        for doc in self.documents:
            # åˆ†è¯æ ‡é¢˜å’Œå†…å®¹
            text = f"{doc['filename']} {doc['content']}"
            tokens = self.tokenize(text)
            self.tokenized_corpus.append(tokens)
        
        bm25 = BM25Okapi(self.tokenized_corpus)
        print(f"  âœ“ Indexed {len(self.documents)} documents for BM25")
        return bm25
    
    def build_vector_index(self) -> np.ndarray:
        """æ„å»ºå‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨ Ollamaï¼‰"""
        print("ğŸ§  Building vector index with Ollama...")
        
        embeddings = []
        for i, doc in enumerate(self.documents):
            print(f"  Processing {i+1}/{len(self.documents)}: {doc['filename']}", end="\r")
            
            # æå–æ–‡æ¡£å‰1000å­—ç¬¦ä½œä¸º embedding è¾“å…¥
            text = f"{doc['filename']}: {doc['content'][:800]}"
            embedding = self.get_embedding(text)
            
            if embedding:
                embeddings.append(embedding)
            else:
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡å ä½
                print(f"\n  âš ï¸  Failed to get embedding for {doc['filename']}")
                embeddings.append([0.0] * 1024)  # mxbai-embed-large æ˜¯ 1024 ç»´
        
        print(f"\n  âœ“ Generated {len(embeddings)} embeddings")
        return np.array(embeddings, dtype=np.float32)
    
    def save_index(self, bm25: BM25Okapi, embeddings: np.ndarray):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        print("ğŸ’¾ Saving index files...")
        
        # ä¿å­˜ BM25 ç´¢å¼•
        bm25_path = self.index_dir / "bm25_index.pkl"
        with open(bm25_path, "wb") as f:
            pickle.dump({
                "bm25": bm25,
                "tokenized_corpus": self.tokenized_corpus
            }, f)
        print(f"  âœ“ BM25 index saved: {bm25_path}")
        
        # ä¿å­˜å‘é‡ç´¢å¼•
        vector_path = self.index_dir / "vector_index.npy"
        np.save(vector_path, embeddings)
        print(f"  âœ“ Vector index saved: {vector_path}")
        
        # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®ï¼ˆä¸åŒ…å«å®Œæ•´å†…å®¹ï¼Œé¿å…é‡å¤ï¼‰
        metadata = []
        for doc in self.documents:
            metadata.append({
                "id": doc["id"],
                "filename": doc["filename"],
                "path": doc["path"],
                "size": doc["size"],
                "mtime": doc["mtime"],
                "word_count": doc["word_count"]
            })
        
        meta_path = self.index_dir / "metadata.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump({
                "version": "1.0",
                "created_at": datetime.now().isoformat(),
                "document_count": len(self.documents),
                "embedding_dim": embeddings.shape[1] if len(embeddings) > 0 else 0,
                "documents": metadata
            }, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ Metadata saved: {meta_path}")
        
        # ä¿å­˜å®Œæ•´æ–‡æ¡£å†…å®¹ï¼ˆä¾›æœç´¢ä½¿ç”¨ï¼‰
        docs_path = self.index_dir / "documents.json"
        with open(docs_path, "w", encoding="utf-8") as f:
            json.dump(self.documents, f, ensure_ascii=False)
        print(f"  âœ“ Documents saved: {docs_path}")
        
        # è®¡ç®—å¹¶ä¿å­˜ç´¢å¼•å“ˆå¸Œï¼ˆç”¨äºå¢é‡æ›´æ–°æ£€æµ‹ï¼‰
        index_hash = self.compute_index_hash()
        hash_path = self.index_dir / "index.hash"
        hash_path.write_text(index_hash)
        print(f"  âœ“ Index hash saved: {hash_path}")
    
    def compute_index_hash(self) -> str:
        """è®¡ç®—ç´¢å¼•å†…å®¹çš„å“ˆå¸Œå€¼"""
        hasher = hashlib.sha256()
        for doc in sorted(self.documents, key=lambda x: x["id"]):
            hasher.update(f"{doc['id']}:{doc['mtime']}".encode())
        return hasher.hexdigest()[:16]
    
    def check_needs_rebuild(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•"""
        hash_path = self.index_dir / "index.hash"
        
        if not hash_path.exists():
            return True
        
        current_hash = self.compute_index_hash()
        stored_hash = hash_path.read_text().strip()
        
        return current_hash != stored_hash
    
    def build(self, force: bool = False) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„ç´¢å¼•æ„å»ºæµç¨‹"""
        print("=" * 60)
        print("Self-Memory Index Builder")
        print("=" * 60)
        
        # åŠ è½½æ–‡æ¡£
        print(f"\nğŸ“ Loading documents from {self.memory_dir}...")
        self.documents = self.load_documents()
        
        if not self.documents:
            print("âš ï¸  No documents found!")
            return {"success": False, "error": "No documents"}
        
        print(f"  âœ“ Loaded {len(self.documents)} documents")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        if not force:
            needs_rebuild = self.check_needs_rebuild()
            if not needs_rebuild:
                print("\nâœ… Index is up to date. Use --force to rebuild.")
                return {"success": True, "rebuilt": False}
        
        # æ„å»ºç´¢å¼•
        print(f"\nğŸ”¨ Building indexes...")
        bm25 = self.build_bm25_index()
        embeddings = self.build_vector_index()
        
        # ä¿å­˜ç´¢å¼•
        self.save_index(bm25, embeddings)
        
        print(f"\nâœ… Index build complete!")
        print(f"   Documents: {len(self.documents)}")
        print(f"   Embedding dim: {embeddings.shape[1]}")
        print(f"   Index location: {self.index_dir}")
        
        return {
            "success": True,
            "rebuilt": True,
            "document_count": len(self.documents),
            "embedding_dim": embeddings.shape[1]
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Build Self-Memory Index")
    parser.add_argument("--force", action="store_true", help="Force rebuild even if up to date")
    parser.add_argument("--check", action="store_true", help="Check if rebuild is needed")
    
    args = parser.parse_args()
    
    builder = IndexBuilder()
    
    if args.check:
        needs_rebuild = builder.check_needs_rebuild()
        print(f"Index needs rebuild: {needs_rebuild}")
        sys.exit(0 if not needs_rebuild else 1)
    
    result = builder.build(force=args.force)
    sys.exit(0 if result["success"] else 1)


if __name__ == "__main__":
    main()
